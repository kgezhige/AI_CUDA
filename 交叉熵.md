## 交叉熵损失的公式

对于一个单个样本 \( (x, y) \)，其中 \( x \) 是模型的输出（未归一化的对数概率），\( y \) 是真实标签（整数索引），交叉熵损失可以表示为：

$$
\text{CrossEntropyLoss} = -\log\left(\frac{\exp(x_y)}{\sum_{j=1}^{C} \exp(x_j)}\right)
$$

其中：
- \( C \) 是类别的总数。
- \( x_y \) 是模型输出中对应于真实标签 \( y \) 的值。
- 分母是对所有类别输出的指数函数求和，这实际上就是 softmax 函数的分母部分。

### 例子说明

假设我们有一个三分类问题，模型输出是未经归一化的对数概率（logits），并且我们有以下数据：

- 模型输出（logits）: 
  $$
  x = \begin{bmatrix}
  1.2 & 0.3 & 2.5 \\
  0.8 & 1.4 & 0.6
  \end{bmatrix}
  $$
- 真实标签:
  $$
  y = \begin{bmatrix}
  2 \\
  1
  \end{bmatrix}
  $$

我们需要计算每个样本的交叉熵损失，然后取平均值作为最终的损失。

#### 计算过程

1. **应用 LogSoftmax**:
   对于第一个样本（第一行）:
   $$
   \text{LogSoftmax}(x_1) = \log\left(\frac{\exp(1.2)}{\exp(1.2) + \exp(0.3) + \exp(2.5)}\right), \log\left(\frac{\exp(0.3)}{\exp(1.2) + \exp(0.3) + \exp(2.5)}\right), \log\left(\frac{\exp(2.5)}{\exp(1.2) + \exp(0.3) + \exp(2.5)}\right)
   $$

   对于第二个样本（第二行）:
   $$
   \text{LogSoftmax}(x_2) = \log\left(\frac{\exp(0.8)}{\exp(0.8) + \exp(1.4) + \exp(0.6)}\right), \log\left(\frac{\exp(1.4)}{\exp(0.8) + \exp(1.4) + \exp(0.6)}\right), \log\left(\frac{\exp(0.6)}{\exp(0.8) + \exp(1.4) + \exp(0.6)}\right)
   $$

2. **选择对应的真实标签的值**:
   - 第一个样本的真实标签是 2，所以我们选择 `LogSoftmax(x_1)` 中的第三个值。类似于onehot编码 $[0,0,1]$, 我们希望
   $LogSoftmax(x_1)$ 中的第三个值越接近于1越好。
   - 第二个样本的真实标签是 1，所以我们选择 `LogSoftmax(x_2)` 中的第二个值。

3. **计算负对数似然损失**:
   - 第一个样本的损失: \( -\log\left(\frac{\exp(2.5)}{\exp(1.2) + \exp(0.3) + \exp(2.5)}\right) \)
   - 第二个样本的损失: \( -\log\left(\frac{\exp(1.4)}{\exp(0.8) + \exp(1.4) + \exp(0.6)}\right) \)

4. **取平均值**:
   最后，将所有样本的损失取平均值作为最终的损失。
5. 

### 使用 PyTorch 实现

下面是使用 PyTorch 实现上述过程的代码示例：

```python
import torch
import torch.nn as nn

# 模型输出（logits）
outputs = torch.tensor([[1.2, 0.3, 2.5], [0.8, 1.4, 0.6]], requires_grad=True)

# 真实标签
labels = torch.tensor([2, 1])

# 创建 CrossEntropyLoss 对象
criterion = nn.CrossEntropyLoss()

# 计算损失
loss = criterion(outputs, labels)

print(f'Loss: {loss.item()}')

# 反向传播
loss.backward()
```
交叉熵损失（Cross-Entropy Loss）是机器学习和深度学习中用于衡量概率分布差异的核心损失函数，尤其在分类任务中广泛应用。以下是其全面解析：

核心思想

交叉熵量化真实分布与模型预测分布之间的差异。在分类任务中：
真实分布：通常是One-hot编码（如 [0, 0, 1, 0]）

预测分布：模型输出的概率（如 [0.1, 0.2, 0.6, 0.1]）

目标是最小化两者之间的交叉熵，推动预测逼近真实分布。

数学定义

(1) 二分类（Binary Cross-Entropy）

\[
= -\frac{1}{N} \sum_{i=1}^N \left[ y_i \log(p_i) + (1-y_i) \log(1-p_i) \right]

\]
 y_i : 真实标签（0或1）

 p_i : 预测为正类的概率

(2) 多分类（Categorical Cross-Entropy）

\[
= -\frac{1}{N} \sum_{i=1}^N \sum_{c=1}^C y_{i,c} \log(p_{i,c})

\]
 C : 类别数

 y_{i,c} : 样本  i  在类别  c  的真实标签（0或1）

 p_{i,c} : 模型预测样本  i  属于类别  c  的概率

关键特性

特性 说明

非对称性 只关注真实类别对应的预测概率，忽略错误类别的具体分布
概率敏感 对预测概率的微小变化敏感（梯度大 near 0或1）
与Softmax联动 通常搭配Softmax使用，将输出转换为概率分布

为什么用交叉熵？

梯度友好  

   相比均方误差（MSE），交叉熵在错误预测时提供更大梯度，加速收敛。  
   示例：若真实标签为1，预测为0.01时：  
交叉熵梯度 ≈ -100，强烈修正  

MSE梯度 ≈ -0.02，更新缓慢  
信息论基础  

   源自KL散度（相对熵），最小化交叉熵等价于最小化真实分布与预测分布的差异。
分类任务适配  

   直接优化概率输出，而非连续值（如MSE更适合回归）。

PyTorch实现

(1) 二分类

loss_fn = nn.BCELoss()  # 输入需经过sigmoid
或

loss_fn = nn.BCEWithLogitsLoss()  # 自动含sigmoid（数值稳定）

(2) 多分类

loss_fn = nn.CrossEntropyLoss()  # 输入为未归一化的logits（无需手动softmax）
使用示例

logits = model(inputs)  # 形状 (batch_size, num_classes)
loss = loss_fn(logits, labels)  # labels为类别索引（非one-hot）

实际案例

假设3分类任务：
真实标签：[0, 2]（PyTorch格式，类别索引）

模型输出：  

  [[1.2, -0.5, 3.1], [0.8, 2.0, -1.0]]（logits）
Softmax概率：  

  [[0.15, 0.05, 0.80], [0.10, 0.85, 0.05]]
交叉熵计算：  

  \( L = -\frac{1}{2} \left[ \log(0.15) + \log(0.85) \right] \approx 0.86 \)

常见问题

Q1: 为什么不用MSE做分类？
MSE假设噪声高斯分布，分类标签是离散的，且梯度在饱和区（概率接近0/1）消失。

Q2: 如何处理数值不稳定？
使用 LogSoftmax + NLLLoss 或直接 CrossEntropyLoss（PyTorch已优化）。

Q3: 样本不均衡时如何改进？
加权交叉熵：nn.CrossEntropyLoss(weight=class_weights)

交叉熵损失是深度学习分类任务的基石，理解其原理和实现细节对模型调优至关重要！